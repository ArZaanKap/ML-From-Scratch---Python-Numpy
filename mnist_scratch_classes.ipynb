{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0105041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf208fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each row is 1 image\n",
    "# 0 is black, 255 is white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971112f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"mnist_train.csv\")\n",
    "print(len(train_df))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d08e4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"mnist_test.csv\")\n",
    "print(len(test_df))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "591da915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# inspect data\n",
    "idx = 0\n",
    "row = train_df.iloc[idx]\n",
    "\n",
    "img_data = np.array(row.drop(\"label\"))\n",
    "img_data = img_data.reshape(28, 28)\n",
    "img_data = np.expand_dims(img_data, axis=2)\n",
    "\n",
    "label = row[\"label\"]\n",
    "print(f\"label: {label}\")\n",
    "\n",
    "plt.imshow(img_data, cmap='gray')        # need (28,28,1) -> (H,W,C)  for plotting\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "979e5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a1367fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len: 50000\n",
      "val len: 10000\n",
      "test len: 10000\n"
     ]
    }
   ],
   "source": [
    "# make val out of train too\n",
    "train_val = np.array(train_df)\n",
    "\n",
    "# 50k, 10k, 10k\n",
    "train_data = train_val[0:50000, :]\n",
    "val_data = train_val[50000:, :]\n",
    "test_data = np.array(test_df)\n",
    "\n",
    "print(f\"train len: {len(train_data)}\")\n",
    "print(f\"val len: {len(val_data)}\")\n",
    "print(f\"test len: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a30642d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[:, 1:].T\n",
    "y_train = train_data[:, 0]      # vector\n",
    "\n",
    "X_val = val_data[:, 1:].T\n",
    "y_val = val_data[:, 0]\n",
    "\n",
    "X_test = test_data[:, 1:].T\n",
    "y_test = test_data[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcf79078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalisation 0 to 1 instead of 0 to 255\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9142a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders for batches\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, X, y, batch_size, shuffle=False, seed=42):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_features, self.n_samples = X.shape\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle == True:            # remove idices already used\n",
    "            idx = self.rng.permutation(self.n_samples)    # use rng from above\n",
    "        else:\n",
    "            idx = np.arange(self.n_samples)     # [0 to n_samples]\n",
    "\n",
    "        for start in range(0, self.n_samples, self.batch_size):\n",
    "            b = idx[start:start + self.batch_size]\n",
    "            yield self.X[:, b], self.y[b]\n",
    "\n",
    "\n",
    "train_set = DataLoader(X_train, y_train, batch_size=100, shuffle=True, seed=42)\n",
    "val_set = DataLoader(X_val, y_val, batch_size=100, seed=42)\n",
    "test_set = DataLoader(X_test, y_test, batch_size=100, seed=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f786c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=0) # need to sum each col\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    one_hot_y = np.zeros((num_classes, len(y))) # 10 x m\n",
    "\n",
    "    #for idx, val in enumerate(y):\n",
    "        #one_hot_y[val,idx] = 1.0\n",
    "\n",
    "    one_hot_y[y, np.arange(len(y))] = 1.0\n",
    "    \n",
    "    return one_hot_y\n",
    "\n",
    "\n",
    "def get_preds(probs):\n",
    "    return np.argmax(probs, axis=0) # return y_hat\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80913437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Module(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropyLoss(Module):     # (10, 50k)\n",
    "\n",
    "    def __call__(self, logits, y):\n",
    "        return self.forward(logits, y)\n",
    "\n",
    "    def forward(self, logits, y):\n",
    "\n",
    "        self.y = y\n",
    "        self.probs = softmax(logits)\n",
    "        return np.mean(-np.log(self.probs[y,np.arange(len(y))]), axis=0)\n",
    "    \n",
    "    def backward(self):\n",
    "        \n",
    "        return self.probs - one_hot(self.y)\n",
    "\n",
    "\n",
    "class ReLU(Module):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "       \n",
    "        relu_grad = (self.x > 0)\n",
    "        return grad_output * relu_grad\n",
    "    \n",
    "\n",
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.2):  # prob of dropping a neuron\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            self.mask = (np.random.rand(*x.shape) > self.p) / (1 - self.p)  # inverted dropout\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x    # no dropout during inference\n",
    "    \n",
    "    def backward(self, gradient_output):\n",
    "        return gradient_output * self.mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b18a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward requires 3 derivatives\n",
    "# dW, db and dX to feed to previous layer\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "\n",
    "        # He initialisation\n",
    "        self.W = np.random.randn(out_features, in_features) * np.sqrt(2.0/in_features)\n",
    "        self.b = np.zeros((out_features, 1))\n",
    "\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x): # (784, batch)\n",
    "        self.x = x      # store for backprop\n",
    "        return self.W @ x + self.b\n",
    "\n",
    "\n",
    "    def backward(self, grad_output):    # dZ from next layer\n",
    "\n",
    "        m = self.x.shape[1]     # n_samples\n",
    "\n",
    "        self.dW = 1/m * grad_output @ self.x.T\n",
    "        self.db = 1/m * np.sum(grad_output, axis=1, keepdims=True)\n",
    "\n",
    "        grad_input =  self.W.T @ grad_output\n",
    "        return grad_input\n",
    "    \n",
    "    def params(self):\n",
    "        return [(self.W, self.dW, self.b, self.db)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba75e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout2D(Module):\n",
    "    def __init__(self, p=0.1):  # prob of dropping a neuron\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):   # drop entire channels instead of random pixels across all channels\n",
    "        if self.training:\n",
    "            batch, channels, h_in, w_in = x.shape\n",
    "            self.mask = (np.random.rand(batch, channels, 1, 1) > self.p) / (1 - self.p)  # inverted dropout\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x    # no dropout during inference\n",
    "    \n",
    "    def backward(self, gradient_output):\n",
    "        return gradient_output * self.mask\n",
    "\n",
    "\n",
    "class Flatten(Module):\n",
    "    \n",
    "    def forward(self, x):   # (batch, channels, h, w) -> (vector, batch)\n",
    "\n",
    "        self.input_shape = x.shape\n",
    "        return x.reshape(-1, x.shape[0]) # (c*h*w, batch) this is what linear expects as input\n",
    "\n",
    "\n",
    "    def backward(self, grad_out): # (c*h*w, batch) -> (batch, ch, h, w)\n",
    "\n",
    "        return np.transpose(grad_out).reshape(self.input_shape)\n",
    "\n",
    "       \n",
    "\n",
    "class MaxPool(Module):\n",
    "    def __init__(self, factor, stride=2, padding=0):\n",
    "\n",
    "        if isinstance(factor, tuple): # is factor a tuple\n",
    "            self.kernel_h = factor[0]\n",
    "            self.kernel_w = factor[1]\n",
    "        else:\n",
    "            self.kernel_h = factor\n",
    "            self.kernel_w = factor\n",
    "\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        batch_size, channels, h, w = x.shape\n",
    "        P = self.padding\n",
    "\n",
    "        if P > 0:   # not zero in case all negative numbers maxpool will return 0\n",
    "            self.x_padded = np.full((batch_size, channels, h+2*P, w+2*P), -np.inf)\n",
    "            self.x_padded[:, :, P:P+h, P:P+w] = x\n",
    "        else:\n",
    "            self.x_padded = x\n",
    "\n",
    "        out_h = (h - self.kernel_h + 2*self.padding) // self.stride + 1\n",
    "        out_w = (w - self.kernel_w + 2*self.padding) // self.stride + 1\n",
    "    \n",
    "        output = np.full((batch_size, channels, out_h, out_w), -np.inf)  \n",
    "        \n",
    "        # vectorise over batch and channels\n",
    "        for i in range(out_h): \n",
    "            for j in range(out_w): \n",
    "\n",
    "                h_start, w_start = i*self.stride, j*self.stride\n",
    "                h_end, w_end = h_start + self.kernel_h, w_start + self.kernel_w\n",
    "\n",
    "                x_slice = self.x_padded[:,:, h_start:h_end, w_start:w_end]\n",
    "\n",
    "                output[:,:,i,j] = np.max(x_slice, axis=(2,3))\n",
    "            \n",
    "        return output\n",
    "    \n",
    "\n",
    "    def backward(self, grad_out):\n",
    "\n",
    "        out_h, out_w = grad_out.shape[2], grad_out.shape[3]\n",
    "\n",
    "        grad_in = np.zeros_like(self.x_padded)\n",
    "\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "\n",
    "                h_start, w_start = i*self.stride, j*self.stride\n",
    "                h_end, w_end = h_start + self.kernel_h, w_start + self.kernel_w\n",
    "\n",
    "                x_slice = self.x_padded[:,:, h_start:h_end, w_start:w_end]\n",
    "\n",
    "                max_val = np.max(x_slice, axis=(2,3), keepdims=True)\n",
    "                local_mask = (max_val == x_slice)\n",
    "\n",
    "                grad = grad_out[:,:,i,j][:,:,None,None]\n",
    "                grad_in[:,:,h_start:h_end, w_start:w_end] += grad * local_mask\n",
    "\n",
    "        if self.padding > 0:\n",
    "            P = self.padding\n",
    "            return grad_in[:,:,P:-P, P:-P]\n",
    "        \n",
    "        return grad_in\n",
    "\n",
    "\n",
    "class AvgPool(Module):\n",
    "    def __init__(self, factor, stride=2, padding=0):\n",
    "\n",
    "        if isinstance(factor, tuple): # is factor a tuple\n",
    "            self.kernel_h = factor[0]\n",
    "            self.kernel_w = factor[1]\n",
    "        else:\n",
    "            self.kernel_h = factor\n",
    "            self.kernel_w = factor\n",
    "\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        batch_size, channels, h, w = x.shape\n",
    "        P = self.padding\n",
    "\n",
    "        if P > 0:   # zero padded\n",
    "            self.x_padded = np.zeros((batch_size, channels, h+2*P, w+2*P))\n",
    "            self.x_padded[:, :, P:P+h, P:P+w] = x\n",
    "        else:\n",
    "            self.x_padded = x\n",
    "\n",
    "        out_h = (h - self.kernel_h + 2*self.padding) // self.stride + 1\n",
    "        out_w = (w - self.kernel_w + 2*self.padding) // self.stride + 1\n",
    "    \n",
    "        output = np.zeros((batch_size, channels, out_h, out_w))  \n",
    "        \n",
    "        # vectorise over batch and channels\n",
    "        for i in range(out_h): \n",
    "            for j in range(out_w): \n",
    "\n",
    "                h_start, w_start = i*self.stride, j*self.stride\n",
    "                h_end, w_end = h_start + self.kernel_h, w_start + self.kernel_w\n",
    "\n",
    "                x_slice = self.x_padded[:,:, h_start:h_end, w_start:w_end]\n",
    "\n",
    "                output[:,:,i,j] = np.mean(x_slice, axis=(2,3))\n",
    "            \n",
    "        return output\n",
    "    \n",
    "\n",
    "    def backward(self, grad_out):\n",
    "\n",
    "        out_h, out_w = grad_out.shape[2], grad_out.shape[3]\n",
    "\n",
    "        grad_in = np.zeros_like(self.x_padded)\n",
    "\n",
    "        N = self.kernel_h*self.kernel_w\n",
    "\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "\n",
    "                h_start, w_start = i*self.stride, j*self.stride\n",
    "                h_end, w_end = h_start + self.kernel_h, w_start + self.kernel_w\n",
    "\n",
    "                ########?\n",
    "                grad = grad_out[:,:,i,j][:,:,None,None]    # give each pixel equal split of gradient\n",
    "                grad_in[:,:,h_start:h_end, w_start:w_end] += grad/N\n",
    "\n",
    "        if self.padding > 0:\n",
    "            P = self.padding\n",
    "            return grad_in[:,:,P:-P, P:-P]\n",
    "        \n",
    "        return grad_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f022b629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59617056 0.88860609]\n",
      " [0.40833054 0.30313269]\n",
      " [0.5351949  0.75234267]\n",
      " [0.9908327  0.82811997]] \n",
      "\n",
      "[[[[0.59617056 0.40833054]\n",
      "   [0.5351949  0.9908327 ]]]\n",
      "\n",
      "\n",
      " [[[0.88860609 0.30313269]\n",
      "   [0.75234267 0.82811997]]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.rand(4, 2) # vec, b\n",
    "print(x,\"\\n\")\n",
    "y = x.T.reshape(x.shape[1], 1, 2, 2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b95ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Conv2d(Module):   \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        if isinstance(kernel_size, tuple):\n",
    "            self.kernel_h, self.kernel_w = kernel_size[0], kernel_size[1]\n",
    "        else:\n",
    "            self.kernel_h, self.kernel_w = kernel_size\n",
    "        \n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # the kernel - He initialisation\n",
    "        fan_in = in_channels * self.kernel_h * self.kernel_w\n",
    "        self.W = np.random.randn(out_channels, in_channels, self.kernel_h, self.kernel_w) * np.sqrt(2.0 / fan_in)\n",
    "        self.b = np.zeros(out_channels)\n",
    "\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        self.x_padded = None\n",
    "\n",
    "    # x -> (784, 50k)\n",
    "    def forward(self, x):   # In (784, batch) reshape (b, c, h, w)   Output feature map (C_out, H', W')\n",
    "\n",
    "        if x.ndim == 2:\n",
    "                    # (784,b) -> (b,784) -> (b, 1, 28, 28)\n",
    "            self.x = np.transpose(x).reshape(x.shape[1], 1, 28, 28)\n",
    "        else:\n",
    "            self.x = x\n",
    "\n",
    "        batch_size, C_in, height, width = self.x.shape\n",
    "        P = self.padding\n",
    "\n",
    "        if P != 0:\n",
    "            self.x_padded = np.zeros((batch_size, C_in, height+2*P, width+2*P))\n",
    "            self.x_padded[:, :, P:P+height, P:P+width] = self.x\n",
    "        else:\n",
    "            self.x_padded = self.x\n",
    "\n",
    "\n",
    "        new_width = (width - self.kernel_w + 2*P) // self.stride + 1   # changed standard formula since input already padded\n",
    "        new_height = (height - self.kernel_h + 2*P) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((batch_size, self.out_channels, new_height, new_width))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for i in range(new_height):\n",
    "                for j in range(new_width):\n",
    "\n",
    "                    h_start, w_start = i*self.stride, j*self.stride\n",
    "                    h_end, w_end = h_start + self.kernel_h, w_start + self.kernel_w\n",
    "\n",
    "                    x_slice = self.x_padded[b, :, h_start:h_end, w_start:w_end]\n",
    "\n",
    "                    for f in range(self.out_channels):\n",
    "                        output[b, f, i, j] = np.sum(x_slice * self.W[f]) + self.b[f]\n",
    "\n",
    "        return output   # z\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_out):\n",
    "        \n",
    "        # grad_out - (batch, channels, h, w)\n",
    "        batch_size, out_channels, h_out, w_out = grad_out.shape\n",
    "\n",
    "        # init\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        grad_in = np.zeros_like(self.x_padded)\n",
    "\n",
    "        # 1. dL/db\n",
    "        self.db = np.sum(grad_out, axis=(0,2,3)) # one b value for each filter\n",
    "\n",
    "        # 2. dL/dW = conv(x, dL/dz)\n",
    "        # 3. dL/dx\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for i in range(h_out):\n",
    "                for j in range(w_out):\n",
    "\n",
    "                    h_start, w_start = i*self.stride, j*self.stride\n",
    "                    h_end, w_end = h_start + self.kernel_h, w_start + self.kernel_w\n",
    "\n",
    "                    grad_pixel = grad_out[b,:,i,j] # gradient vector for specific pixel for all channels in this image\n",
    "                    x_slice = self.x_padded[b,:, h_start:h_end, w_start:w_end]\n",
    "                    \n",
    "                    for f in range(self.out_channels):\n",
    "                        grad = grad_pixel[f]\n",
    "\n",
    "                        # dW update\n",
    "                        self.dW[f] += grad * x_slice\n",
    "\n",
    "                        # dX update\n",
    "                        grad_in[b,:, h_start:h_end, w_start:w_end] += self.W[f] * grad\n",
    "        \n",
    "        # normalise\n",
    "        self.dW = self.dW / batch_size\n",
    "        self.db = self.db / batch_size\n",
    "\n",
    "        if self.padding > 0:\n",
    "            P = self.padding\n",
    "            return grad_in[:,:,P:-P, P:-P]\n",
    "        \n",
    "        return grad_in\n",
    "    \n",
    "\n",
    "    def params(self):\n",
    "        return [(self.W, self.dW, self.b, self.db)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f185d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, layers): \n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):      # dont need to do .forward()\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output)\n",
    "        return grad_output\n",
    "    \n",
    "    def train(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'training'):\n",
    "                layer.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'training'):\n",
    "                layer.training = False\n",
    "\n",
    "    def load_params(self, saved_params):\n",
    "        param_idx = 0\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'params'):\n",
    "                W, b = saved_params[param_idx]\n",
    "                layer.W = W.copy()\n",
    "                layer.b = b.copy()\n",
    "                param_idx += 1\n",
    "\n",
    "    def params(self):\n",
    "        all_params = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'params'):\n",
    "                all_params.extend(layer.params())\n",
    "                \n",
    "        return all_params\n",
    "    \n",
    "    # [\n",
    "    #   (W1, dW1, b1, db1), \n",
    "    #   (W2, dW2, b2, db2),   \n",
    "    # ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, model, lr, weight_decay=0.0, momentum=0.9):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.velocities = None\n",
    "\n",
    "    def step(self):\n",
    "        params = self.model.params()\n",
    "\n",
    "        # init velocities on first step\n",
    "        if self.velocities is None and self.momentum > 0.0:\n",
    "            self.velocities = []\n",
    "            for W, dW, b, db in params:\n",
    "                self.velocities.append({\n",
    "                    'W': np.zeros_like(W),\n",
    "                    'b': np.zeros_like(b)\n",
    "                })\n",
    "\n",
    "        for i, (W, dW, b, db) in enumerate(params):\n",
    "\n",
    "            if self.momentum > 0.0: \n",
    "                self.velocities[i]['W'] = self.momentum * self.velocities[i]['W'] + (1-self.momentum)*dW\n",
    "                self.velocities[i]['b'] = self.momentum * self.velocities[i]['b'] + (1-self.momentum)*db\n",
    "                \n",
    "                W -= self.lr * (self.velocities[i]['W'] + self.weight_decay * W)\n",
    "                b -= self.lr * self.velocities[i]['b']\n",
    "\n",
    "            else:\n",
    "                W -= self.lr * (dW + self.weight_decay * W)\n",
    "                b -= self.lr * db\n",
    "\n",
    "\n",
    "class AdamW:\n",
    "    def __init__(self, model, lr, weight_decay=0.0):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.m = None   # first moment - mean\n",
    "        self.v = None   # second moment - variance\n",
    "        self.b1 = 0.9\n",
    "        self.b2 = 0.999\n",
    "        self.eps = 1e-8\n",
    "\n",
    "        self.t = 1\n",
    "\n",
    "    def step(self):\n",
    "        params = self.model.params()\n",
    "\n",
    "        # init velocities and s on first step\n",
    "        if self.m is None and self.v is None:\n",
    "            self.m = []\n",
    "            self.v = []\n",
    "            for W, dW, b, db in params:\n",
    "                self.m.append({'W': np.zeros_like(W), 'b': np.zeros_like(b)})\n",
    "                self.v.append({'W': np.zeros_like(W), 'b': np.zeros_like(b)})\n",
    "\n",
    "\n",
    "        for i, (W, dW, b, db) in enumerate(params):\n",
    "\n",
    "            self.m[i]['W'] = self.b1 * self.m[i]['W'] + (1-self.b1)*dW\n",
    "            self.m[i]['b'] = self.b1 * self.m[i]['b'] + (1-self.b1)*db\n",
    "\n",
    "            self.v[i]['W'] = self.b2 * self.v[i]['W'] + (1-self.b2) * dW**2\n",
    "            self.v[i]['b'] = self.b2 * self.v[i]['b'] + (1-self.b2) * db**2\n",
    "            \n",
    "            # bias correction\n",
    "            m_hat_W = self.m[i]['W'] / (1 - self.b1**self.t)\n",
    "            m_hat_b = self.m[i]['b'] / (1 - self.b1**self.t)\n",
    "            v_hat_W = self.v[i]['W'] / (1 - self.b2**self.t)\n",
    "            v_hat_b = self.v[i]['b'] / (1 - self.b2**self.t)\n",
    "\n",
    "            W -= self.lr * (m_hat_W / (np.sqrt(v_hat_W) + self.eps) + self.weight_decay * W)\n",
    "            b -= self.lr * m_hat_b / (np.sqrt(v_hat_b) + self.eps)\n",
    "        \n",
    "        self.t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a2dc210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   m = β1 * m + (1 - β1) * gradient           # momentum (like SGD momentum)\n",
    "#   v = β2 * v + (1 - β2) * gradient²          # tracks gradient variance\n",
    "\n",
    "#   m_hat = m / (1 - β1^t)                     # bias correction (early steps)\n",
    "#   v_hat = v / (1 - β2^t)\n",
    "\n",
    "#   W -= lr * m_hat / (√v_hat + ε)             # adaptive learning rate per param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbce09d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent(model, optimizer, criterion, train_dataset, val_dataset, num_iter):\n",
    "\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_iter):\n",
    "\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        # train\n",
    "        for X, y in train_dataset:\n",
    "\n",
    "            # forward\n",
    "            logits = model(X)\n",
    "\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            total_train_loss += loss\n",
    "\n",
    "            # backward\n",
    "            grad_out = loss.backward() # loss\n",
    "            model.backward(grad_out)\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            y_hat = get_preds(logits) # same as taking argmax of probs\n",
    "            train_correct += (y_hat==y).sum()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        total_val_loss = 0.0\n",
    "        \n",
    "        # val\n",
    "        for X, y in val_dataset:\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            total_val_loss += loss\n",
    "\n",
    "            y_hat = get_preds(logits)\n",
    "            val_correct += (y_hat==y).sum()\n",
    "\n",
    "        avg_train_loss = total_train_loss / (train_dataset.n_samples // train_dataset.batch_size)###\n",
    "        train_acc = train_correct / train_dataset.n_samples * 100.0\n",
    "\n",
    "        avg_val_loss = total_val_loss / (val_dataset.n_samples // val_dataset.batch_size)###\n",
    "        val_acc = val_correct / val_dataset.n_samples * 100.0\n",
    "\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch}  train loss: {avg_train_loss:.4f}    train acc: {train_acc:.2f}%     val loss: {avg_val_loss:.4f}    val acc: {val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "            best_params = [(W.copy(), b.copy()) for W, _, b, _ in model.params()]    # save best weights\n",
    "        \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f125aae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  train loss: 0.3050    train acc: 90.66%     val loss: 0.1371    val acc: 95.93%\n",
      "Epoch: 10  train loss: 0.1173    train acc: 96.48%     val loss: 0.1310    val acc: 97.18%\n",
      "Epoch: 20  train loss: 0.0955    train acc: 97.30%     val loss: 0.1892    val acc: 97.06%\n",
      "Epoch: 30  train loss: 0.0886    train acc: 97.61%     val loss: 0.1884    val acc: 97.31%\n",
      "Epoch: 40  train loss: 0.0831    train acc: 97.77%     val loss: 0.1958    val acc: 97.00%\n",
      "Epoch: 50  train loss: 0.0820    train acc: 97.94%     val loss: 0.2203    val acc: 97.35%\n",
      "Epoch: 60  train loss: 0.0794    train acc: 98.08%     val loss: 0.2414    val acc: 97.06%\n",
      "Epoch: 70  train loss: 0.0698    train acc: 98.31%     val loss: 0.2945    val acc: 97.10%\n",
      "Epoch: 80  train loss: 0.0741    train acc: 98.26%     val loss: 0.2863    val acc: 97.25%\n",
      "Epoch: 90  train loss: 0.0653    train acc: 98.47%     val loss: 0.3247    val acc: 96.93%\n",
      "Epoch: 100  train loss: 0.0646    train acc: 98.51%     val loss: 0.3599    val acc: 97.03%\n",
      "Epoch: 110  train loss: 0.0591    train acc: 98.62%     val loss: 0.3614    val acc: 97.42%\n",
      "Epoch: 120  train loss: 0.0645    train acc: 98.65%     val loss: 0.3387    val acc: 97.26%\n",
      "Epoch: 130  train loss: 0.0584    train acc: 98.61%     val loss: 0.3545    val acc: 97.34%\n",
      "Epoch: 140  train loss: 0.0678    train acc: 98.59%     val loss: 0.3962    val acc: 96.89%\n",
      "Epoch: 150  train loss: 0.0717    train acc: 98.54%     val loss: 0.4354    val acc: 96.97%\n",
      "Epoch: 160  train loss: 0.0688    train acc: 98.59%     val loss: 0.3781    val acc: 97.20%\n",
      "Epoch: 170  train loss: 0.0606    train acc: 98.72%     val loss: 0.4158    val acc: 97.12%\n",
      "Epoch: 180  train loss: 0.0584    train acc: 98.82%     val loss: 0.4402    val acc: 97.31%\n",
      "Epoch: 190  train loss: 0.0685    train acc: 98.67%     val loss: 0.4480    val acc: 97.20%\n",
      "Epoch: 200  train loss: 0.0650    train acc: 98.65%     val loss: 0.5263    val acc: 96.99%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#model_dir = 'scratch_models'\n",
    "#os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Linear(784, 100),\n",
    "    ReLU(),\n",
    "    Dropout(0.2),\n",
    "    Linear(100, 10)\n",
    "])\n",
    "\n",
    "#optimizer = SGD(model=model, lr=1.5, weight_decay=0.0, momentum=0.9)\n",
    "optimizer = Adam(model=model, lr=0.01, weight_decay=0)\n",
    "criterion = SoftmaxCrossEntropyLoss()\n",
    "\n",
    "# training\n",
    "best_params = gradient_descent(model, optimizer, criterion, train_set, val_set, num_iter=201) # get final weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de1e7f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 97.22%\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "\n",
    "model.eval()\n",
    "model.load_params(best_params)\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for X, y in test_set:\n",
    "    logits = model.forward(X)\n",
    "    probs = softmax(logits)\n",
    "    \n",
    "    y_hat_test = get_preds(probs)\n",
    "\n",
    "    correct += (y_hat_test==y).sum()\n",
    "\n",
    "test_acc = correct / test_set.n_samples * 100.0\n",
    "\n",
    "print(f\"test acc: {test_acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
