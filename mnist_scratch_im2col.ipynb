{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f0105041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cf208fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each row is 1 image\n",
    "# 0 is black, 255 is white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "971112f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"mnist_train.csv\")\n",
    "print(len(train_df))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8d08e4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"mnist_test.csv\")\n",
    "print(len(test_df))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "591da915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# inspect data\n",
    "idx = 0\n",
    "row = train_df.iloc[idx]\n",
    "\n",
    "img_data = np.array(row.drop(\"label\"))\n",
    "img_data = img_data.reshape(28, 28)\n",
    "img_data = np.expand_dims(img_data, axis=2)\n",
    "\n",
    "label = row[\"label\"]\n",
    "print(f\"label: {label}\")\n",
    "\n",
    "plt.imshow(img_data, cmap='gray')        # need (28,28,1) -> (H,W,C)  for plotting\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "979e5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3a1367fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len: 50000\n",
      "val len: 10000\n",
      "test len: 10000\n"
     ]
    }
   ],
   "source": [
    "# make val out of train too\n",
    "train_val = np.array(train_df)\n",
    "\n",
    "# 50k, 10k, 10k\n",
    "train_data = train_val[0:50000, :]\n",
    "val_data = train_val[50000:, :]\n",
    "test_data = np.array(test_df)\n",
    "\n",
    "print(f\"train len: {len(train_data)}\")\n",
    "print(f\"val len: {len(val_data)}\")\n",
    "print(f\"test len: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a30642d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[:, 1:].T\n",
    "y_train = train_data[:, 0]      # vector\n",
    "\n",
    "X_val = val_data[:, 1:].T\n",
    "y_val = val_data[:, 0]\n",
    "\n",
    "X_test = test_data[:, 1:].T\n",
    "y_test = test_data[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fcf79078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalisation 0 to 1 instead of 0 to 255\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9142a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders for batches\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, X, y, batch_size, shuffle=False, seed=42, img_shape=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.img_shape = img_shape  # (1, 28, 28)\n",
    "\n",
    "        self.n_features, self.n_samples = X.shape   # (features, n_samples)\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.dataset = []\n",
    "\n",
    "        for i in range(self.n_samples):\n",
    "\n",
    "            if img_shape is None:\n",
    "                self.dataset.append((X[:, i], y[i]))\n",
    "            \n",
    "            else:\n",
    "                self.dataset.append((X[:, i].reshape(*img_shape), y[i]))\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle == True:            # remove idices already used\n",
    "            idx = self.rng.permutation(self.n_samples)    # use rng from above\n",
    "        else:\n",
    "            idx = np.arange(self.n_samples)     # [0 to n_samples]\n",
    "\n",
    "        for start in range(0, self.n_samples, self.batch_size):\n",
    "            b = idx[start:start + self.batch_size]\n",
    "            X_batch = self.X[:, b]\n",
    "\n",
    "            if self.img_shape is not None:\n",
    "                # (features, batch) -> (batch, features) -> (b, c, h, w)\n",
    "                X_batch = np.transpose(X_batch).reshape(self.batch_size, *self.img_shape)\n",
    "\n",
    "            yield X_batch, self.y[b]\n",
    "\n",
    "\n",
    "img_shape = (1, 28, 28) # or None for MLP\n",
    "\n",
    "train_loader = DataLoader(X_train, y_train, batch_size=100, shuffle=True, seed=42, img_shape=img_shape)\n",
    "val_loader = DataLoader(X_val, y_val, batch_size=100, seed=42, img_shape=img_shape)\n",
    "test_loader = DataLoader(X_test, y_test, batch_size=100, seed=42, img_shape=img_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5f786c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stable version\n",
    "def softmax(z):\n",
    "    z_stable = z - np.max(z, axis=0, keepdims=True)\n",
    "    return np.exp(z_stable) / np.sum(np.exp(z_stable), axis=0) # need to sum each col\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    one_hot_y = np.zeros((num_classes, len(y))) # 10 x m\n",
    "\n",
    "    #for idx, val in enumerate(y):\n",
    "        #one_hot_y[val,idx] = 1.0\n",
    "\n",
    "    one_hot_y[y, np.arange(len(y))] = 1.0\n",
    "    \n",
    "    return one_hot_y\n",
    "\n",
    "\n",
    "def get_preds(probs):\n",
    "    return np.argmax(probs, axis=0) # return y_hat\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80913437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8418bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Module(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropyLoss(Module):     # (10, 50k)\n",
    "\n",
    "    def __call__(self, logits, y):\n",
    "        return self.forward(logits, y)\n",
    "\n",
    "    def forward(self, logits, y):\n",
    "\n",
    "        self.y = y\n",
    "        self.probs = softmax(logits)\n",
    "        return np.mean(-np.log(self.probs[y,np.arange(len(y))] + 1e-8), axis=0)\n",
    "    \n",
    "    def backward(self):\n",
    "        \n",
    "        return self.probs - one_hot(self.y)\n",
    "\n",
    "\n",
    "class ReLU(Module):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "       \n",
    "        relu_grad = (self.x > 0)\n",
    "        return grad_output * relu_grad\n",
    "    \n",
    "\n",
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.2):  # prob of dropping a neuron\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            self.mask = (np.random.rand(*x.shape) > self.p) / (1 - self.p)  # inverted dropout\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x    # no dropout during inference\n",
    "    \n",
    "    def backward(self, gradient_output):\n",
    "        return gradient_output * self.mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0a2b18a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward requires 3 derivatives\n",
    "# dW, db and dX to feed to previous layer\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "\n",
    "        # He initialisation\n",
    "        self.W = np.random.randn(out_features, in_features) * np.sqrt(2.0/in_features)\n",
    "        self.b = np.zeros((out_features, 1))\n",
    "\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x): # (784, batch)\n",
    "        self.x = x      # store for backprop\n",
    "        return self.W @ x + self.b\n",
    "\n",
    "\n",
    "    def backward(self, grad_output):    # dZ from next layer\n",
    "\n",
    "        m = self.x.shape[1]     # n_samples\n",
    "\n",
    "        self.dW = 1/m * grad_output @ self.x.T\n",
    "        self.db = 1/m * np.sum(grad_output, axis=1, keepdims=True)\n",
    "\n",
    "        grad_input =  self.W.T @ grad_output\n",
    "        return grad_input\n",
    "    \n",
    "    def params(self):\n",
    "        return [(self.W, self.dW, self.b, self.db)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e9ba75e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout2D(Module):\n",
    "    def __init__(self, p=0.1):  # prob of dropping a neuron\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):   # drop entire channels instead of random pixels across all channels\n",
    "        if self.training:\n",
    "            batch, channels, h_in, w_in = x.shape\n",
    "            self.mask = (np.random.rand(batch, channels, 1, 1) > self.p) / (1 - self.p)  # inverted dropout\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x    # no dropout during inference\n",
    "    \n",
    "    def backward(self, gradient_output):\n",
    "        return gradient_output * self.mask\n",
    "\n",
    "\n",
    "class Flatten(Module):\n",
    "    \n",
    "    def forward(self, x):   # (batch, channels, h, w) -> (vector, batch)\n",
    "\n",
    "        self.input_shape = x.shape\n",
    "        batch = x.shape[0]\n",
    "\n",
    "        return x.reshape(batch, -1).T  # (c*h*w, batch) this is what linear expects as input\n",
    "\n",
    "\n",
    "    def backward(self, grad_out): # (c*h*w, batch) -> (batch, ch, h, w)\n",
    "\n",
    "        return np.transpose(grad_out).reshape(self.input_shape)\n",
    "\n",
    "       \n",
    "\n",
    "class MaxPool(Module):\n",
    "    def __init__(self, factor, stride=2, padding=0):\n",
    "\n",
    "        if isinstance(factor, tuple): # is factor a tuple\n",
    "            self.kernel_h = factor[0]\n",
    "            self.kernel_w = factor[1]\n",
    "        else:\n",
    "            self.kernel_h = factor\n",
    "            self.kernel_w = factor\n",
    "\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        b, c, h, w = x.shape\n",
    "        P = self.padding\n",
    "        s = self.stride\n",
    "\n",
    "        kh, kw = self.kernel_h, self.kernel_w\n",
    "\n",
    "        if P > 0:   # not zero in case all negative numbers maxpool will return 0\n",
    "            x_padded = np.full((b, c, h+2*P, w+2*P), -np.inf)\n",
    "            x_padded[:, :, P:P+h, P:P+w] = x\n",
    "        else:\n",
    "            x_padded = x\n",
    "\n",
    "        _,_, h_pad, w_pad = x_padded.shape\n",
    "\n",
    "        self.out_h = (h - kh + 2*P) // s + 1\n",
    "        self.out_w = (w - kw + 2*P) // s + 1\n",
    "    \n",
    "        # index arrays\n",
    "        h_idx = (np.arange(self.out_h) * s).reshape(-1,1) + np.arange(kh)  # (out_h, kh)\n",
    "        w_idx = (np.arange(self.out_w) * s).reshape(-1,1) + np.arange(kw)  # (out_w, kw)\n",
    "        \n",
    "        # flatten to 1d indices - (out_h, 1, kh, 1) * w_pad + (1, out_w, 1, kw)\n",
    "        flat_idx = h_idx[:, None, :, None] * w_pad + w_idx[None, :, None, :]   # (out_h, out_w, kh, kw)\n",
    "\n",
    "        # extract all windows\n",
    "        x_flat = x_padded.reshape(b, c, -1)\n",
    "        windows = x_flat[:,:, flat_idx.flatten()].reshape(b, c, self.out_h, self.out_w, kh*kw)\n",
    "\n",
    "        self.max_idx = np.argmax(windows, axis=4)\n",
    "        self.flat_idx = flat_idx\n",
    "        self.h_pad, self.w_pad = h_pad, w_pad\n",
    "\n",
    "        return np.max(windows, axis=4)\n",
    "    \n",
    "\n",
    "    def backward(self, grad_out):\n",
    "\n",
    "        b, c, out_h, out_w = grad_out.shape\n",
    "        kh, kw = self.kernel_h, self.kernel_w\n",
    "        p = self.padding\n",
    "\n",
    "        # create grad at max positions\n",
    "        grad_windows = np.zeros((b, c, out_h, out_w, kh*kw))\n",
    "\n",
    "        # scatter grad_out to max indices\n",
    "        b_idx, c_idx, out_h_idx, out_w_idx = np.indices((b, c, out_h, out_w))\n",
    "        grad_windows[b_idx, c_idx, out_h_idx, out_w_idx, self.max_idx] = grad_out\n",
    "\n",
    "        # scatter back to input shape\n",
    "        grad_flat = np.zeros((b, c, self.h_pad * self.w_pad))\n",
    "        np.add.at(grad_flat, (slice(None), slice(None), self.flat_idx.flatten()), \n",
    "                  grad_windows.reshape(b, c, -1))\n",
    "\n",
    "        grad_padded = grad_flat.reshape(b, c, self.h_pad, self.w_pad)\n",
    "\n",
    "        if p > 0:\n",
    "            return grad_padded[:,:, p:-p, p:-p]\n",
    "        return grad_padded\n",
    "    \n",
    "\n",
    "\n",
    "class AvgPool(Module):\n",
    "    def __init__(self, factor, stride=2, padding=0):\n",
    "\n",
    "        if isinstance(factor, tuple): # is factor a tuple\n",
    "            self.kernel_h = factor[0]\n",
    "            self.kernel_w = factor[1]\n",
    "        else:\n",
    "            self.kernel_h = factor\n",
    "            self.kernel_w = factor\n",
    "\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        batch_size, channels, h, w = x.shape\n",
    "        P = self.padding\n",
    "\n",
    "        if P > 0:   # zero padded\n",
    "            self.x_padded = np.zeros((batch_size, channels, h+2*P, w+2*P))\n",
    "            self.x_padded[:, :, P:P+h, P:P+w] = x\n",
    "        else:\n",
    "            self.x_padded = x\n",
    "\n",
    "        out_h = (h - self.kernel_h + 2*self.padding) // self.stride + 1\n",
    "        out_w = (w - self.kernel_w + 2*self.padding) // self.stride + 1\n",
    "    \n",
    "        output = np.zeros((batch_size, channels, out_h, out_w))  \n",
    "        \n",
    "        # vectorise over batch and channels\n",
    "        for i in range(out_h): \n",
    "            for j in range(out_w): \n",
    "\n",
    "                h_start, w_start = i*self.stride, j*self.stride\n",
    "                h_end, w_end = h_start + self.kernel_h, w_start + self.kernel_w\n",
    "\n",
    "                x_slice = self.x_padded[:,:, h_start:h_end, w_start:w_end]\n",
    "\n",
    "                output[:,:,i,j] = np.mean(x_slice, axis=(2,3))\n",
    "            \n",
    "        return output\n",
    "    \n",
    "\n",
    "    def backward(self, grad_out):\n",
    "\n",
    "        out_h, out_w = grad_out.shape[2], grad_out.shape[3]\n",
    "\n",
    "        grad_in = np.zeros_like(self.x_padded)\n",
    "\n",
    "        N = self.kernel_h*self.kernel_w\n",
    "\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "\n",
    "                h_start, w_start = i*self.stride, j*self.stride\n",
    "                h_end, w_end = h_start + self.kernel_h, w_start + self.kernel_w\n",
    "\n",
    "                ########?\n",
    "                grad = grad_out[:,:,i,j][:,:,None,None]    # give each pixel equal split of gradient\n",
    "                grad_in[:,:,h_start:h_end, w_start:w_end] += grad/N\n",
    "\n",
    "        if self.padding > 0:\n",
    "            P = self.padding\n",
    "            return grad_in[:,:,P:-P, P:-P]\n",
    "        \n",
    "        return grad_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2b95ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Conv2d(Module):   \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        if isinstance(kernel_size, tuple):\n",
    "            self.kernel_h, self.kernel_w = kernel_size\n",
    "        else:\n",
    "            self.kernel_h = kernel_size\n",
    "            self.kernel_w = kernel_size\n",
    "        \n",
    "\n",
    "        # the kernel - He initialisation\n",
    "        fan_in = in_channels * self.kernel_h * self.kernel_w\n",
    "        self.W = np.random.randn(out_channels, fan_in) * np.sqrt(2.0 / fan_in)\n",
    "        self.b = np.zeros((out_channels,1))\n",
    "\n",
    "        self.W_shape = (out_channels, in_channels, self.kernel_h, self.kernel_w)\n",
    "\n",
    "        self.x_cols = None\n",
    "        self.x_shape = None\n",
    "\n",
    "    # x -> (784, 50k)\n",
    "    def forward(self, x):   # In (784, batch) reshape (b, c, h, w)   Output feature map (C_out, H', W')\n",
    "\n",
    "        self.x = x\n",
    "\n",
    "        batch_size, C, H, W = self.x.shape\n",
    "        P = self.padding\n",
    "\n",
    "        self.h_out = (H - self.kernel_h + 2*P) // self.stride + 1\n",
    "        self.w_out = (W - self.kernel_w + 2*P) // self.stride + 1\n",
    "\n",
    "        # im2col - large matrix where each col is 3D patch from x (h,w,c)\n",
    "        self.x_cols = self.im2col_indices()\n",
    "\n",
    "        # conv becomes matrix multiplication\n",
    "        output = self.W @ self.x_cols + self.b\n",
    "\n",
    "        # reshape back to 4D img\n",
    "        # (C_out, b * h_out * w_out) -> (C_out, h_out, w_out, b) -> (b, C_out, h_out, w_out)\n",
    "        output = output.reshape(self.out_channels, self.h_out, self.w_out, batch_size)\n",
    "        output = output.transpose(3, 0, 1, 2)\n",
    "\n",
    "        return output   # z\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_out):   # (b, c_out, h_out, w_out)\n",
    "\n",
    "        m = grad_out.shape[0]   # for averaging\n",
    "\n",
    "        # reshape grad to match matmul shape\n",
    "        # Transpose to (c_out, h_out, w_out, b)  flatten (c_out, -1)\n",
    "\n",
    "        grad_out = grad_out.transpose(1, 2, 3, 0).reshape(self.out_channels, -1)\n",
    "        \n",
    "        # dW = grad_out * x_cols.T\n",
    "        self.dW = 1/m * (grad_out @ self.x_cols.T)\n",
    "        #self.dW = self.dW.reshape(self.W_shape)\n",
    "\n",
    "        # db\n",
    "        self.db = 1/m * np.sum(grad_out, axis=1, keepdims=True) # sum across channels\n",
    "\n",
    "        # dx_cols = W.T @ grad_out\n",
    "        dX_cols = self.W.T @ grad_out\n",
    "\n",
    "        grad_in = self.col2im_indices(dX_cols)\n",
    "\n",
    "        return grad_in\n",
    "    \n",
    "\n",
    "    def im2col_indices(self):\n",
    "        # zero pad input\n",
    "        b, C, h, w = self.x.shape\n",
    "        p = self.padding\n",
    "\n",
    "        if self.padding == 0:\n",
    "            x_padded = self.x\n",
    "        else:\n",
    "            x_padded = np.zeros((b, C, h + 2*p, w + 2*p))\n",
    "            x_padded[:,:, p:-p, p:-p] = self.x\n",
    "\n",
    "        k, i, j = self.get_im2col_indices()\n",
    "\n",
    "        cols = x_padded[:, k, i, j]\n",
    "\n",
    "        cols = cols.transpose(1,2,0).reshape(self.kernel_h * self.kernel_w * C, -1)\n",
    "        return cols\n",
    "        \n",
    "    \n",
    "    def col2im_indices(self, dX_cols):\n",
    "        b, C, h, w = self.x.shape\n",
    "        p = self.padding\n",
    "\n",
    "        h_pad, w_pad = h + 2*p, w + 2*p\n",
    "        x_padded = np.zeros((b, C, h_pad, w_pad)) #dtype?\n",
    "\n",
    "        k, i, j = self.get_im2col_indices()\n",
    "\n",
    "        cols = dX_cols.reshape(C * self.kernel_h * self.kernel_w, -1, b)\n",
    "        cols = cols.transpose(2, 0, 1)\n",
    "\n",
    "        np.add.at(x_padded, (slice(None), k, i, j), cols)\n",
    "\n",
    "        if self.padding == 0:\n",
    "            return x_padded\n",
    "        return x_padded[:,:, p:-p, p:-p]\n",
    "\n",
    "    \n",
    "    def get_im2col_indices(self):\n",
    "        b, C, h, w = self.x.shape\n",
    "\n",
    "        i0 = np.repeat(np.arange(self.kernel_h), self.kernel_w)\n",
    "        i0 = np.tile(i0, C)\n",
    "\n",
    "        i1 = self.stride * np.repeat(np.arange(self.h_out), self.w_out)\n",
    "\n",
    "        j0 = np.tile(np.arange(self.kernel_w), self.kernel_h * C)\n",
    "        j1 = self.stride * np.tile(np.arange(self.w_out), self.h_out)\n",
    "\n",
    "        i = i0.reshape(-1,1) + i1.reshape(1,-1)\n",
    "        j = j0.reshape(-1,1) + j1.reshape(1,-1)\n",
    "\n",
    "        k = np.repeat(np.arange(C), self.kernel_h*self.kernel_w).reshape(-1,1)\n",
    "\n",
    "        return k, i, j\n",
    "        \n",
    "\n",
    "    def params(self):\n",
    "        return [(self.W, self.dW, self.b, self.db)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f185d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, layers): \n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):      # dont need to do .forward()\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output)\n",
    "        return grad_output\n",
    "    \n",
    "    def train(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'training'):\n",
    "                layer.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'training'):\n",
    "                layer.training = False\n",
    "\n",
    "    def load_params(self, saved_params):\n",
    "        param_idx = 0\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'params'):\n",
    "                W, b = saved_params[param_idx]\n",
    "                layer.W = W.copy()\n",
    "                layer.b = b.copy()\n",
    "                param_idx += 1\n",
    "\n",
    "    def params(self):\n",
    "        all_params = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'params'):\n",
    "                all_params.extend(layer.params())\n",
    "                \n",
    "        return all_params\n",
    "    \n",
    "    # [\n",
    "    #   (W1, dW1, b1, db1), \n",
    "    #   (W2, dW2, b2, db2),   \n",
    "    # ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8c0b79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, model, lr, weight_decay=0.0, momentum=0.9):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.velocities = None\n",
    "\n",
    "    def step(self):\n",
    "        params = self.model.params()\n",
    "\n",
    "        # init velocities on first step\n",
    "        if self.velocities is None and self.momentum > 0.0:\n",
    "            self.velocities = []\n",
    "            for W, dW, b, db in params:\n",
    "                self.velocities.append({\n",
    "                    'W': np.zeros_like(W),\n",
    "                    'b': np.zeros_like(b)\n",
    "                })\n",
    "\n",
    "        for i, (W, dW, b, db) in enumerate(params):\n",
    "\n",
    "            if self.momentum > 0.0: \n",
    "                self.velocities[i]['W'] = self.momentum * self.velocities[i]['W'] + (1-self.momentum)*dW\n",
    "                self.velocities[i]['b'] = self.momentum * self.velocities[i]['b'] + (1-self.momentum)*db\n",
    "                \n",
    "                W -= self.lr * (self.velocities[i]['W'] + self.weight_decay * W)\n",
    "                b -= self.lr * self.velocities[i]['b']\n",
    "\n",
    "            else:\n",
    "                W -= self.lr * (dW + self.weight_decay * W)\n",
    "                b -= self.lr * db\n",
    "\n",
    "\n",
    "class AdamW:\n",
    "    def __init__(self, model, lr, weight_decay=0.0):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.m = None   # first moment - mean\n",
    "        self.v = None   # second moment - variance\n",
    "        self.b1 = 0.9\n",
    "        self.b2 = 0.999\n",
    "        self.eps = 1e-8\n",
    "\n",
    "        self.t = 1\n",
    "\n",
    "    def step(self):\n",
    "        params = self.model.params()\n",
    "\n",
    "        # init velocities and s on first step\n",
    "        if self.m is None and self.v is None:\n",
    "            self.m = []\n",
    "            self.v = []\n",
    "            for W, dW, b, db in params:\n",
    "                self.m.append({'W': np.zeros_like(W), 'b': np.zeros_like(b)})\n",
    "                self.v.append({'W': np.zeros_like(W), 'b': np.zeros_like(b)})\n",
    "\n",
    "\n",
    "        for i, (W, dW, b, db) in enumerate(params):\n",
    "\n",
    "            self.m[i]['W'] = self.b1 * self.m[i]['W'] + (1-self.b1)*dW\n",
    "            self.m[i]['b'] = self.b1 * self.m[i]['b'] + (1-self.b1)*db\n",
    "\n",
    "            self.v[i]['W'] = self.b2 * self.v[i]['W'] + (1-self.b2) * dW**2\n",
    "            self.v[i]['b'] = self.b2 * self.v[i]['b'] + (1-self.b2) * db**2\n",
    "            \n",
    "            # bias correction\n",
    "            m_hat_W = self.m[i]['W'] / (1 - self.b1**self.t)\n",
    "            m_hat_b = self.m[i]['b'] / (1 - self.b1**self.t)\n",
    "            v_hat_W = self.v[i]['W'] / (1 - self.b2**self.t)\n",
    "            v_hat_b = self.v[i]['b'] / (1 - self.b2**self.t)\n",
    "\n",
    "            W -= self.lr * (m_hat_W / (np.sqrt(v_hat_W) + self.eps) + self.weight_decay * W)\n",
    "            b -= self.lr * m_hat_b / (np.sqrt(v_hat_b) + self.eps)\n",
    "        \n",
    "        self.t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7a2dc210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   m = β1 * m + (1 - β1) * gradient           # momentum (like SGD momentum)\n",
    "#   v = β2 * v + (1 - β2) * gradient²          # tracks gradient variance\n",
    "\n",
    "#   m_hat = m / (1 - β1^t)                     # bias correction (early steps)\n",
    "#   v_hat = v / (1 - β2^t)\n",
    "\n",
    "#   W -= lr * m_hat / (√v_hat + ε)             # adaptive learning rate per param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dbce09d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent(model, optimizer, criterion, train_loader, val_loader, num_iter):\n",
    "\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_iter):\n",
    "\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        # train\n",
    "        for X, y in train_loader:\n",
    "\n",
    "            # forward\n",
    "            logits = model(X)\n",
    "\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            total_train_loss += loss\n",
    "\n",
    "            # backward\n",
    "            grad_out = criterion.backward() # loss\n",
    "            model.backward(grad_out)\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            y_hat = get_preds(logits) # same as taking argmax of probs\n",
    "            train_correct += (y_hat==y).sum()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        total_val_loss = 0.0\n",
    "        \n",
    "        # val\n",
    "        for X, y in val_loader:\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            total_val_loss += loss\n",
    "\n",
    "            y_hat = get_preds(logits)\n",
    "            val_correct += (y_hat==y).sum()\n",
    "\n",
    "        avg_train_loss = total_train_loss / (train_loader.n_samples // train_loader.batch_size)###\n",
    "        train_acc = train_correct / train_loader.n_samples * 100.0\n",
    "\n",
    "        avg_val_loss = total_val_loss / (val_loader.n_samples // val_loader.batch_size)###\n",
    "        val_acc = val_correct / val_loader.n_samples * 100.0\n",
    "\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch: {epoch}  train loss: {avg_train_loss:.4f}    train acc: {train_acc:.2f}%     val loss: {avg_val_loss:.4f}    val acc: {val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "            best_params = [(W.copy(), b.copy()) for W, _, b, _ in model.params()]    # save best weights\n",
    "        \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f125aae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  train loss: 0.1965    train acc: 94.01%     val loss: 0.0726    val acc: 97.82%\n",
      "Epoch: 1  train loss: 0.0892    train acc: 97.29%     val loss: 0.0555    val acc: 98.35%\n",
      "Epoch: 2  train loss: 0.0786    train acc: 97.50%     val loss: 0.0709    val acc: 97.74%\n",
      "Epoch: 3  train loss: 0.0680    train acc: 97.85%     val loss: 0.0545    val acc: 98.23%\n",
      "Epoch: 4  train loss: 0.0631    train acc: 98.10%     val loss: 0.0550    val acc: 98.47%\n",
      "Epoch: 5  train loss: 0.0643    train acc: 97.99%     val loss: 0.0703    val acc: 97.92%\n",
      "Epoch: 6  train loss: 0.0643    train acc: 97.99%     val loss: 0.0569    val acc: 98.48%\n",
      "Epoch: 7  train loss: 0.0584    train acc: 98.21%     val loss: 0.0643    val acc: 98.07%\n",
      "Epoch: 8  train loss: 0.0623    train acc: 98.05%     val loss: 0.0485    val acc: 98.66%\n",
      "Epoch: 9  train loss: 0.0591    train acc: 98.23%     val loss: 0.0591    val acc: 98.42%\n",
      "Epoch: 10  train loss: 0.0565    train acc: 98.22%     val loss: 0.0528    val acc: 98.57%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#model_dir = 'scratch_models'\n",
    "#os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "model1 = Sequential([\n",
    "    Linear(784, 100),\n",
    "    ReLU(),\n",
    "    Dropout(0.2),\n",
    "    Linear(100, 10)\n",
    "])\n",
    "\n",
    "\n",
    "model2 = Sequential([\n",
    "    Conv2d(1, 2, kernel_size=(5,5), padding=2),\n",
    "    ReLU(),\n",
    "\n",
    "    Conv2d(2, 4, kernel_size=(5,5), padding=2),\n",
    "    ReLU(),\n",
    "    MaxPool(2),\n",
    "\n",
    "    Conv2d(4, 8, kernel_size=(5,5), padding=2),\n",
    "    ReLU(),\n",
    "    Dropout2D(),\n",
    "    MaxPool(2),\n",
    "\n",
    "    Flatten(),\n",
    "    Linear(8*7*7, 10) # 2 downpools 28/2/2 = 7\n",
    "])\n",
    "\n",
    "#optimizer = SGD(model=model, lr=1.5, weight_decay=0.0, momentum=0.9)\n",
    "optimizer = AdamW(model=model2, lr=0.001, weight_decay=0)\n",
    "criterion = SoftmaxCrossEntropyLoss()\n",
    "\n",
    "# training\n",
    "best_params = gradient_descent(model2, optimizer, criterion, train_loader, val_loader, num_iter=11) # get final weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "de1e7f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 98.34%\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "\n",
    "model = model2\n",
    "\n",
    "model.eval()\n",
    "model.load_params(best_params)\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for X, y in test_loader:\n",
    "    logits = model(X)\n",
    "    \n",
    "    y_hat_test = get_preds(logits)\n",
    "\n",
    "    correct += (y_hat_test==y).sum()\n",
    "\n",
    "test_acc = correct / test_loader.n_samples * 100.0\n",
    "\n",
    "print(f\"test acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b50ba4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: [0]\n",
      "label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualise\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# X_test  (n_features, n_samples)\n",
    "\n",
    "idx = 3\n",
    "x, y = test_loader.dataset[idx]     # x (1, 28, 28)\n",
    "\n",
    "logits = model(x.reshape(1, *x.shape)) # add batch dim\n",
    "pred = get_preds(logits)\n",
    "\n",
    "print(f\"pred: {pred}\")\n",
    "print(f\"label: {y}\")\n",
    "\n",
    "img = x.squeeze(0)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
